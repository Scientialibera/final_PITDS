{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main notebook for the project\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_station_links(url):\n",
    "    urls = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.ok:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', text='View data')\n",
    "            for link in links:\n",
    "                urls.append(link['href'])\n",
    "        else:\n",
    "            print(f\"Error accessing page: Status code {response.status_code}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error during requests to {url} : {str(e)}\")\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def create_folder_if_not_exists(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "\n",
    "def download_file(url, folder):\n",
    "\n",
    "    # Extract the file name from the URL\n",
    "    file_name = url.split('/')[-1]\n",
    "\n",
    "    # Create the full path for the file\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    \n",
    "    # Download and save the file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "def find_data_start_row(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Check if the line starts with space, indicating the start of the data\n",
    "            if line.startswith(\"   \"):\n",
    "                return i  # Return the row number for the data start\n",
    "    return 0  # Return 0 if no data start row is found\n",
    "\n",
    "def txt_to_csv(file_path, output_folder):\n",
    "\n",
    "    start_row = find_data_start_row(file_path)\n",
    "\n",
    "    df = pd.read_csv(file_path, skiprows=start_row, delim_whitespace=True, usecols=[0, 1, 2, 3, 4, 5, 6])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "    output_file = os.path.join(output_folder, file_name.replace('.txt', '.csv'))\n",
    "  \n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    return output_folder\n",
    "\n",
    "def download_main_dataset_1(station_url, output_folder_txt):\n",
    "    # Extract CSV links from the given station URL\n",
    "    csv_links = extract_station_links(station_url)\n",
    "\n",
    "    # Create directories for TXT and CSV files if they don't exist\n",
    "    create_folder_if_not_exists(output_folder_txt)\n",
    "\n",
    "    # Download and convert each file in csv_links\n",
    "    for link in csv_links:\n",
    "        download_file(link, output_folder_txt)\n",
    "\n",
    "def convert_txts_to_csvs(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    # Convert each file in the input folder to CSV\n",
    "    for file in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        if file_path.endswith('.txt'):\n",
    "            txt_to_csv(file_path, output_folder)\n",
    "\n",
    "def clean_and_rename_csv_in_folder(input_folder, output_folder):\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.drop(0).reset_index(drop=True)\n",
    "            df.columns = ['yyyy', 'mm', 'tmax (degC)', 'tmin (degC)', 'af (days)', 'rain (mm)', 'sun (hours)']\n",
    "            df.replace('---', pd.NA, inplace=True)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "# Download and convert the files\n",
    "output_folder_txt = '../data/stations_txt'\n",
    "output_folder_csv = '../data/stations_csv'\n",
    "output_folder_csc_clean = '../data/stations_csv_clean'\n",
    "station_url = 'https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data'\n",
    "\n",
    "#download_main_dataset_1(station_url, output_folder_txt)\n",
    "#convert_txts_to_csvs(output_folder_txt, output_folder_csv)\n",
    "clean_and_rename_csv_in_folder(output_folder_csv, output_folder_csc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
