{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main notebook for the project\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import shutil\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def create_folder_if_not_exists(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "def download_file(url, folder):\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")        \n",
    "\n",
    "def find_data_start_row(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.startswith(\"   \"):\n",
    "                return i\n",
    "    return 0\n",
    "\n",
    "def extract_links(url, href_contains):\n",
    "    urls = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.ok:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                if href_contains in link['href']:\n",
    "                    full_url = urljoin(url, link['href'])\n",
    "                    urls.append(full_url)\n",
    "        else:\n",
    "            print(f\"Error accessing page: Status code {response.status_code}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error during requests to {url} : {str(e)}\")\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def download_main_dataset(url, output_folder, pattern):\n",
    "    timestamp_suffix = datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    output_folder_with_timestamp = f\"{output_folder}{timestamp_suffix}\"\n",
    "    csv_links = extract_links(url, pattern)\n",
    "    create_folder_if_not_exists(output_folder_with_timestamp)\n",
    "\n",
    "    for link in csv_links:\n",
    "        # Pass the modified folder path with the timestamp to the download function\n",
    "        download_file(link, output_folder_with_timestamp)\n",
    "        \n",
    "def txt_to_csv(file_path, output_folder):    #Dataset 1\n",
    "    start_row = find_data_start_row(file_path)\n",
    "    df = pd.read_csv(file_path, skiprows=start_row, delim_whitespace=True, usecols=[0, 1, 2, 3, 4, 5, 6])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    output_file = os.path.join(output_folder, file_name.replace('.txt', '.csv'))\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return output_folder\n",
    "\n",
    "def convert_txts_to_csvs(input_folder, output_folder):    #Dataset 1\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        if file_path.endswith('.txt'):\n",
    "            txt_to_csv(file_path, output_folder)\n",
    "\n",
    "def clean_and_rename_csv_in_folder(input_folder, output_folder, remove_rows_criteria=[], special_chars_to_remove=[]):   #Dataset 1\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.drop(0).reset_index(drop=True)\n",
    "            df.columns = ['yyyy', 'mm', 'tmax (degC)', 'tmin (degC)', 'af (days)', 'rain (mm)', 'sun (hours)']\n",
    "            df.replace('---', pd.NA, inplace=True)\n",
    "            for criterion in remove_rows_criteria:\n",
    "                for col_name, value in criterion.items():\n",
    "                    df = df[df[col_name] != value]\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':\n",
    "                    for char in special_chars_to_remove:\n",
    "                        df[col] = df[col].str.replace(char, \"\", regex=True)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "def extract_and_save_coordinates(input_folder, output_folder):  #Dataset 1\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if file_path.endswith('.txt'):\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "            lat_lon_pattern = re.compile(r'Lat\\s*([+-]?\\d+\\.\\d+)\\s*Lon\\s*([+-]?\\d+\\.\\d+)')\n",
    "            matches = lat_lon_pattern.findall(text)\n",
    "            if matches:\n",
    "                latest_lat, latest_lon = matches[-1]\n",
    "            else:\n",
    "                latest_lat, latest_lon = None, None\n",
    "            df = pd.DataFrame({\n",
    "                'File Name': [file_name],\n",
    "                'Latitude': [latest_lat],\n",
    "                'Longitude': [latest_lon]\n",
    "            })\n",
    "            output_file_name = os.path.splitext(file_name)[0] + '_coordinates.csv'\n",
    "            output_file_path = os.path.join(output_folder, output_file_name)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "def process_happiness_excel(file_path, sheet_name, word):   #Dataset 2\n",
    "    # Load the CSV data, ignoring the first column which is just an index\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=2)\n",
    "\n",
    "    col_names = [\n",
    "        \"Codes\", \"Area names\", \"Region\", \"Location\", \"Total % 0-4\", \"Total % 5-6\", \"Total % 7-8\", \"Total % 9-10\", \n",
    "        \"Total Average rating\", \"Total Standard deviation\", \"0-4 CV\", \"0-4 Lower limit\", \"0-4 Upper limit\", \n",
    "        \"5-6 CV\", \" 5-6 Lower limit\", \"5-6 Upper limit\", \"7-8 CV\", \"7-8 Lower limit\", \"7-8 Upper limit\", \n",
    "        \"9-10 CV\", \"9-10 Lower limit\", \"9-10 Upper limit\", \"Total CV\", \"Total Lower limit\", \"Total Upper limit\", \n",
    "        \"Sample size\"\n",
    "    ]\n",
    "    for i in range(len(data)):\n",
    "        if word in str(data.iloc[i, 0]):\n",
    "            data.columns = data.iloc[i]\n",
    "            data = data.iloc[i+1:]\n",
    "            break\n",
    "    data = data.dropna(how=\"all\")\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    if \"2012\" in file_path:\n",
    "        new_columns = col_names\n",
    "    else:\n",
    "        del col_names[9]\n",
    "        new_columns = col_names\n",
    "    data.columns = new_columns[:len(data.columns)]\n",
    "\n",
    "    return data\n",
    "\n",
    "def process__excel_folder(input_folder, output_folder, sheet_name, word):   #Dataset 2\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".xls\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            updated_filename = filename[:-3] + \"xlsx\"\n",
    "            updated_file_path = os.path.join(input_folder, updated_filename)\n",
    "            shutil.copy(file_path, updated_file_path)\n",
    "            updated_file_path = os.path.join(input_folder, updated_filename)\n",
    "            processed_data = process_happiness_excel(updated_file_path, sheet_name, word)\n",
    "            output_file_path = os.path.join(output_folder, filename.replace(\".xls\", \".csv\"))\n",
    "            processed_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "def clean_census_rows(input_folder, output_folder):   #Dataset 2\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df[~df['Codes'].str.contains(' ', na=True)]\n",
    "            df.replace('x', np.nan, inplace=True)\n",
    "            df.replace('#', np.nan, inplace=True)\n",
    "            df.replace('x ', np.nan, inplace=True)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "def combine_dataset_2_average(input_folder, output_folder, drop_columns=[\"Total Standard deviation\"], no_group=False):\n",
    "    all_codes_dfs = []\n",
    "    \n",
    "    # Step 1: Collect 'Codes' from each file\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.replace('x', np.nan, inplace=True)\n",
    "            all_codes_dfs.append(df[['Codes']].drop_duplicates())\n",
    "    \n",
    "    # Step 2: Find distinct 'Codes'\n",
    "    distinct_codes_df = pd.concat(all_codes_dfs).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    all_data_dfs = []\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df.replace('x', np.nan, inplace=True)\n",
    "            df = df.drop(columns=drop_columns, errors='ignore')\n",
    "            all_data_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames vertically and sort by 'Codes'\n",
    "    concatenated_df = pd.concat(all_data_dfs, axis=0).sort_values(by='Codes')\n",
    "    concatenated_df[\"Sample size\"] = concatenated_df[\"Sample size\"].astype(float)\n",
    "\n",
    "    # If no_group is True, skip the grouping and averaging process\n",
    "    if no_group:\n",
    "        final_df = concatenated_df\n",
    "    else:\n",
    "        # Step 5: Compute final values based on your criteria\n",
    "        def compute_final_values(group):\n",
    "            numeric_cols = group.select_dtypes(include=np.number).columns\n",
    "            non_numeric_cols = group.select_dtypes(exclude=np.number).columns.drop('Codes')\n",
    "\n",
    "            # For numeric columns, calculate mean\n",
    "            group[numeric_cols] = group[numeric_cols].mean()\n",
    "\n",
    "            # For non-numeric columns, keep the first value\n",
    "            group[non_numeric_cols] = group[non_numeric_cols].apply(lambda x: x.dropna().head(1).item() if not x.dropna().empty else np.nan)\n",
    "\n",
    "            return group.head(1)\n",
    "\n",
    "        final_df = concatenated_df.groupby('Codes', as_index=False).apply(compute_final_values).reset_index(drop=True)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Define the output file path within the output folder\n",
    "    output_file_path = os.path.join(output_folder, \"combined_dataset.csv\" if not no_group else \"merged_dataset.csv\")\n",
    "\n",
    "    # Save the final DataFrame\n",
    "    final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "def impute_missing_values(file_path, columns_to_impute, group_by=None):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if group_by:\n",
    "        for col in columns_to_impute:\n",
    "            df[col] = df.groupby(group_by)[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    else:\n",
    "        for col in columns_to_impute:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_missing_values_in_folder(input_folder, output_folder, columns_to_impute, group_by=None):\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if file_path.endswith('.csv'):\n",
    "            try:\n",
    "                df = impute_missing_values(file_path, columns_to_impute, group_by)\n",
    "                output_file_path = os.path.join(output_folder, file_name)\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred in file: {file_name}\")\n",
    "                print(str(e))\n",
    "\n",
    "def generate_weather_dataset(input_folder, output_folder):\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_data.append(df)\n",
    "    combined_df = pd.concat(all_data, axis=0)\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    output_file_path = os.path.join(output_folder, \"weather_dataset.csv\")\n",
    "    combined_df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1 Download and Schema Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and convert the files for dataset 1\n",
    "output_folder_ds1_txt = '../data/ds1/landing/stations_txt'\n",
    "output_folder_ds1_csv = '../data/ds1/landing/stations_csv'\n",
    "output_folder_ds1_csv_clean = '../data/ds1/bronze/stations_csv_clean'\n",
    "station_url = 'https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data'\n",
    "pattern_station = '.txt'\n",
    "\n",
    "download_main_dataset(station_url, output_folder_ds1_txt, pattern_station)\n",
    "convert_txts_to_csvs(output_folder_ds1_txt, output_folder_ds1_csv)\n",
    "clean_and_rename_csv_in_folder(output_folder_ds1_csv, output_folder_ds1_csv_clean, [{'yyyy': \"Site\"}], ['#', '\\*', 'Change', '\\$', 'all'])\n",
    "\n",
    "# Extract and save coordinates\n",
    "output_folder_ds1_coordinates = '../data/ds1/reference/locations'\n",
    "extract_and_save_coordinates(output_folder_ds1_txt, output_folder_ds1_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2 Download and Schema Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and convert the files for dataset 2\n",
    "output_folder_ds2_xlsx = '../data/ds2/landing/census_xlsx'\n",
    "output_folder_ds2_csv = '../data/ds2/landing/census_csv'\n",
    "output_folder_ds2_csv_clean = '../data/ds2/bronze/census_csv_clean'\n",
    "output_folder_ds2_csv_combined ='../data/ds2/bronze/census_csv_combined'\n",
    "census_url = \"https://www.ons.gov.uk/peoplepopulationandcommunity/wellbeing/datasets/personalwellbeingestimatesgeographicalbreakdown\"\n",
    "pattern_census = 'tcm'\n",
    "create_folder_if_not_exists(output_folder_ds2_csv_combined)\n",
    "download_main_dataset(census_url, output_folder_ds2_xlsx, pattern_census)\n",
    "process__excel_folder(output_folder_ds2_xlsx, output_folder_ds2_csv, 'Happiness', 'Codes')\n",
    "clean_census_rows(output_folder_ds2_csv, output_folder_ds2_csv_clean)\n",
    "combine_dataset_2_average(output_folder_ds2_csv_clean, output_folder_ds2_csv_combined, no_group=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datset 1 Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_ds1_csv_clean = '../data/ds1/bronze/stations_csv_clean'\n",
    "output_folder_ds1_csv_clean_imputed = '../data/ds1/silver/stations_csv_clean_imputed'\n",
    "columns_to_impute = [\"mm\", \"tmax (degC)\", \"tmin (degC)\", \"af (days)\", \"rain (mm)\", \"sun (hours)\"]\n",
    "\n",
    "impute_missing_values_in_folder(output_folder_ds1_csv_clean, output_folder_ds1_csv_clean_imputed, columns_to_impute, group_by=\"mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_and_save_coordinates(output_folder_ds1_txt, '../data/ds1/reference/locations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
