{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main notebook for the project\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import shutil\n",
    "\n",
    "def create_folder_if_not_exists(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "def download_file(url, folder):\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")        \n",
    "\n",
    "def find_data_start_row(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.startswith(\"   \"):\n",
    "                return i\n",
    "    return 0\n",
    "\n",
    "def extract_links(url, href_contains):\n",
    "    urls = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.ok:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                if href_contains in link['href']:\n",
    "                    full_url = urljoin(url, link['href'])  # Correctly use urljoin here\n",
    "                    urls.append(full_url)\n",
    "        else:\n",
    "            print(f\"Error accessing page: Status code {response.status_code}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error during requests to {url} : {str(e)}\")\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def download_main_dataset(url, output_folder, pattern):\n",
    "    csv_links = extract_links(url, pattern)\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for link in csv_links:\n",
    "        download_file(link, output_folder)\n",
    "        \n",
    "def txt_to_csv(file_path, output_folder):    #Dataset 1\n",
    "    start_row = find_data_start_row(file_path)\n",
    "    df = pd.read_csv(file_path, skiprows=start_row, delim_whitespace=True, usecols=[0, 1, 2, 3, 4, 5, 6])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    output_file = os.path.join(output_folder, file_name.replace('.txt', '.csv'))\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return output_folder\n",
    "\n",
    "def convert_txts_to_csvs(input_folder, output_folder):    #Dataset 1\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        if file_path.endswith('.txt'):\n",
    "            txt_to_csv(file_path, output_folder)\n",
    "\n",
    "def clean_and_rename_csv_in_folder(input_folder, output_folder):    #Dataset 1\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.drop(0).reset_index(drop=True)\n",
    "            df.columns = ['yyyy', 'mm', 'tmax (degC)', 'tmin (degC)', 'af (days)', 'rain (mm)', 'sun (hours)']\n",
    "            df.replace('---', pd.NA, inplace=True)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "def process_happiness_excel(file_path, sheet_name, word):\n",
    "    # Load the CSV data, ignoring the first column which is just an index\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=2)\n",
    "\n",
    "    col_names = [\n",
    "        \"Codes\", \"Area names\", \"Region\", \"Location\", \"Total % 0-4\", \"Total % 5-6\", \"Total % 7-8\", \"Total % 9-10\", \n",
    "        \"Total Average rating\", \"Total Standard deviation\", \"0-4 CV\", \"0-4 Lower limit\", \"0-4 Upper limit\", \n",
    "        \"5-6 CV\", \" 5-6 Lower limit\", \"5-6 Upper limit\", \"7-8 CV\", \"7-8 Lower limit\", \"7-8 Upper limit\", \n",
    "        \"9-10 CV\", \"9-10 Lower limit\", \"9-10 Upper limit\", \"Total CV\", \"Total Lower limit\", \"Total Upper limit\", \n",
    "        \"Sample size\"\n",
    "    ]\n",
    "    for i in range(len(data)):\n",
    "        if word in str(data.iloc[i, 0]):\n",
    "            data.columns = data.iloc[i]\n",
    "            data = data.iloc[i+1:]\n",
    "            break\n",
    "    data = data.dropna(how=\"all\")\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.columns = data.columns.str.strip()\n",
    "    if \"2012\" in file_path:\n",
    "        new_columns = col_names\n",
    "    else:\n",
    "        del col_names[9]\n",
    "        new_columns = col_names\n",
    "    data.columns = new_columns[:len(data.columns)]\n",
    "\n",
    "    return data\n",
    "\n",
    "def process__excel_folder(input_folder, output_folder, sheet_name, word):\n",
    "    create_folder_if_not_exists(output_folder)\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".xls\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            updated_filename = filename[:-3] + \"xlsx\"\n",
    "            updated_file_path = os.path.join(input_folder, updated_filename)\n",
    "            shutil.copy(file_path, updated_file_path)\n",
    "            updated_file_path = os.path.join(input_folder, updated_filename)\n",
    "            processed_data = process_happiness_excel(updated_file_path, sheet_name, word)\n",
    "            output_file_path = os.path.join(output_folder, filename.replace(\".xls\", \".csv\"))\n",
    "            processed_data.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1 Download and Schema Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and convert the files for dataset 1\n",
    "output_folder_ds1_txt = '../data/ds1/stations_txt'\n",
    "output_folder_ds1_csv = '../data/ds1/stations_csv'\n",
    "output_folder_ds1_csv_clean = '../data/ds1/stations_csv_clean'\n",
    "station_url = 'https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data'\n",
    "pattern_station = '.txt'\n",
    "\n",
    "download_main_dataset(station_url, output_folder_ds1_txt, pattern_station)\n",
    "convert_txts_to_csvs(output_folder_ds1_txt, output_folder_ds1_csv)\n",
    "clean_and_rename_csv_in_folder(output_folder_ds1_csv, output_folder_ds1_csv_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2 Download and Schema Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: 20112012referencetabletcm77332122.xls\n",
      "Processed and saved: 20122013referencetable_tcm77-332116.xls\n",
      "Processed and saved: geographicbreakdownreferencetable_tcm77-417203.xls\n",
      "Processed and saved: referencetable1geographicalbreakdown_tcm77-378058.xls\n"
     ]
    }
   ],
   "source": [
    "# Download and convert the files for dataset 2\n",
    "output_folder_ds2_xlsx = '../data/ds2/census_xlsx'\n",
    "output_folder_ds2_csv = '../data/ds2/census_csv'\n",
    "output_folder_ds2_csv_clean = '../data/ds2/census_csv_clean'\n",
    "census_url = \"https://www.ons.gov.uk/peoplepopulationandcommunity/wellbeing/datasets/personalwellbeingestimatesgeographicalbreakdown\"\n",
    "pattern_census = 'tcm'\n",
    "\n",
    "download_main_dataset(census_url, output_folder_ds2_xlsx, pattern_census)\n",
    "process__excel_folder(output_folder_ds2_xlsx, output_folder_ds2_csv, 'Happiness', 'Codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "process__excel_folder(output_folder_ds2_xlsx, output_folder_ds2_csv, 'Happiness', 'Codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_happiness_excel(file_path, sheet_name, word):\n",
    "    # Load the CSV data, ignoring the first column which is just an index\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=2)\n",
    "\n",
    "    col_names = [\n",
    "        \"Codes\", \"Area names\", \"Region\", \"Location\", \"Total % 0-4\", \"Total % 5-6\", \"Total % 7-8\", \"Total % 9-10\", \n",
    "        \"Total Average rating\", \"Total Standard deviation\", \"0-4 CV\", \"0-4 Lower limit\", \"0-4 Upper limit\", \n",
    "        \"5-6 CV\", \" 5-6 Lower limit\", \"5-6 Upper limit\", \"7-8 CV\", \"7-8 Lower limit\", \"7-8 Upper limit\", \n",
    "        \"9-10 CV\", \"9-10 Lower limit\", \"9-10 Upper limit\", \"Total CV\", \"Total Lower limit\", \"Total Upper limit\", \n",
    "        \"Sample size\"\n",
    "    ]\n",
    "\n",
    "    # Find the first row where the first cell contains the word and use it as the header\n",
    "    for i in range(len(data)):\n",
    "        if word in str(data.iloc[i, 0]):\n",
    "            data.columns = data.iloc[i]\n",
    "            data = data.iloc[i+1:]\n",
    "            break\n",
    "\n",
    "    # Drop any rows that are completely NaN\n",
    "    data = data.dropna(how=\"all\")\n",
    "\n",
    "    # Reset the index after dropping rows\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Remove empty spaces from column names\n",
    "    data.columns = data.columns.str.strip()\n",
    "\n",
    "\n",
    "    if \"2012\" in file_path:\n",
    "        new_columns = col_names\n",
    "    else:\n",
    "        del col_names[9]\n",
    "        new_columns = col_names\n",
    "\n",
    "    # Assign the new column names to the dataframe\n",
    "    data.columns = new_columns[:len(data.columns)]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\n",
    "        \"Codes\", \"Area names\", \"Region\", \"Location\", \"0-4\", \"5-6\", \"7-8\", \"9-10\", \n",
    "        \"Average rating\", \"Standard deviation\", \"CV\", \"Lower limit\", \"Upper limit\", \n",
    "        \"CV\", \"Lower limit\", \"Upper limit\", \"CV\", \"Lower limit\", \"Upper limit\", \n",
    "        \"CV\", \"Lower limit\", \"Upper limit\", \"CV\", \"Lower limit\", \"Upper limit\", \n",
    "        \"Sample size\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Codes',\n",
       " 'Area names',\n",
       " 'Region',\n",
       " 'Location',\n",
       " '0-4',\n",
       " '5-6',\n",
       " '7-8',\n",
       " '9-10',\n",
       " 'Average rating',\n",
       " 'CV',\n",
       " 'Lower limit',\n",
       " 'Upper limit',\n",
       " 'CV',\n",
       " 'Lower limit',\n",
       " 'Upper limit',\n",
       " 'CV',\n",
       " 'Lower limit',\n",
       " 'Upper limit',\n",
       " 'CV',\n",
       " 'Lower limit',\n",
       " 'Upper limit',\n",
       " 'CV',\n",
       " 'Lower limit',\n",
       " 'Upper limit',\n",
       " 'Sample size']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "del c[9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
